{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from configparser import ConfigParser as cparser\n",
    "cfg = cparser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[path]\n",
      "  new_tweets_dir=\n",
      "  clean_words_dir=\n",
      "  table_path=\n",
      "  log_dir=\n",
      "  banner_dir=\n",
      "\n",
      "[streamer]\n",
      "  access_token=\n",
      "  access_token_secret=\n",
      "  consumer_key=\n",
      "  consumer_secret=\n",
      "  n_bytes_per_tweet=1000\n",
      "  n_tweets_per_file=500\n",
      "  check_in_interval=1000\n",
      "  en_track_words=a, the, with, I, my, and, like\n",
      "\n",
      "[cleaner]\n",
      "  en_buzz_words=\n",
      "\n",
      "[fileserver]\n",
      "  server_address=\n",
      "  server_port=\n",
      "  buffer_size=\n",
      "\n",
      "[parser]\n",
      "  buffer_size=\n",
      "\n",
      "[monitor]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg.read('./tweetspy/tweetspy.ini')\n",
    "for section in cfg.sections():\n",
    "    print(\"[\", section, \"]\", sep='')\n",
    "    for value in cfg.items(section):\n",
    "        print(\"  \", value[0], \"=\", value[1], sep='')\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.getint('fileserver', 'port')\n",
    "cfg.get('fileserver', 'address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter,defaultdict\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords, treebank\n",
    "\n",
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "import numpy.random as rand\n",
    "\n",
    "import string, re, json, socket, sys, os, h5py, nltk,csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f =open('/home/sol/CLUSTER_RAID/Tweets/2016MayTue17-144454.json','r')\n",
    "tweets=[]\n",
    "trash=[]\n",
    "for line in f.readlines():\n",
    "    if len(line) > 9:\n",
    "        tweets.append(line)\n",
    "    else:\n",
    "        trash.append(line)\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time ='Mon May 16 02:33:57 +0000 2016'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general = ['gw', 'le', 'lo', 'll', 'lm', 'li', 'tn', 'tl', 'ls', 'th', \n",
    "           \n",
    "           'ti', 'te', 'do', 'dj', 'yo', 'ya', 'dg', 'yb', 'da', 'dy', \n",
    "           \n",
    "           'uy', 'ys', 'ahaha', 'dp', 'pffttt', 'l', 'btw', 'ea', 'et', \n",
    "           \n",
    "           'rt', 'ul', 'rf', 'rm', 'ro', 'rj', 'wd', 'omg', 'ba', 'wa', \n",
    "           \n",
    "           'ju', 'bn', 'wk', 'bi', 'wi', 'bk', 'wtf', 'bs', 'wy', 'om', \n",
    "           \n",
    "           'oa', 'uni', 'ck', 'vid', 'cl', 'xc', 'ca', 'cf', 'cr', 'pr', \n",
    "           \n",
    "           'pp', 'pa', 'pi', 'tk', 'hr', 'hi', 'ha', 'md', 'ma', 'ml', \n",
    "           \n",
    "           'mi', 'us', 'mt', 'mv', 'ms', 'mr', 'ue', 'ae', 'ad', 'ak', 'vn', \n",
    "           \n",
    "           'ay', 'vr', 'ar', 'ia', 'ie', 'ig', 'nb', 'ny', 'nt', 'fr', 'ft',\n",
    "           \n",
    "           'fu', 'fa', 'fd', 'fe', 'fi', 'fl', 'sfeh', 'ki', 'kn', 'sk', 'kp',\n",
    "           \n",
    "           'sn', 'sl', 'sf','nd', 'lk', 'gd']\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "\n",
    "general_upper = [word.upper() for word in general]\n",
    "\n",
    "general_cap = [word.title() for word in general]\n",
    "\n",
    "stop = json.dumps(stopwords.words('english') )\n",
    "\n",
    "stop = stop.replace('[','').replace('\"','').replace(']','').replace(' ','')\n",
    "\n",
    "stop = stop.split(',')\n",
    "\n",
    "stop_upper = [word.upper() for word in stop]\n",
    "\n",
    "stop_cap = [word.title() for word in stop]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "letters = [letter for letter in \n",
    "           \n",
    "           string.ascii_lowercase + \n",
    "           \n",
    "           string.ascii_uppercase \n",
    "           \n",
    "           if letter not in \n",
    "           \n",
    "           ['a','A','I','i']]\n",
    "\n",
    "nums = []\n",
    "\n",
    "for i in range(10000):\n",
    "\n",
    "    nums.append(str(i))\n",
    "\n",
    "    nums.append(str(i/1e1))\n",
    "\n",
    "\n",
    "all_stops = general + general_upper + general_cap + letters + punctuation + stop + nums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "\n",
    "    (?:\n",
    "\n",
    "        [:=;] # Eyes\n",
    "\n",
    "        [oO\\-]? # Nose (optional)\n",
    "\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "\n",
    "    )\"\"\"\n",
    "\n",
    "regex_str = [\n",
    "\n",
    "    emoticons_str,\n",
    "\n",
    "    r'<[^>]+>', # HTML tags\n",
    "\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "\n",
    "    r'(?:\\S)' # anything else\n",
    "\n",
    "        ]\n",
    "\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "\n",
    "def tokenize(s):\n",
    "    \n",
    "    return word_tokenize(s) \n",
    "\n",
    "def preprocess(s, lowercase=False):\n",
    "\n",
    "    tokens = tokenize(s)\n",
    "    \n",
    "    if lowercase:\n",
    "        \n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clean_List_of_Sentence(sent_list):\n",
    "    \n",
    "    keepers = []\n",
    "    \n",
    "    for i in sent_list:\n",
    "\n",
    "        for j in i:\n",
    "            \n",
    "\n",
    "            if j in all_stops:\n",
    "\n",
    "                continue\n",
    "\n",
    "            for i in punctuation:\n",
    "\n",
    "                j.replace(i,'').replace(i+i,'').replace(i+i+i,'')\n",
    "\n",
    "            if j.startswith( ('#','@','http','//','/','~',':','\\\\n','\\\\')):\n",
    "\n",
    "                continue\n",
    "\n",
    "            if j.endswith( ('#','@','http','//','/','~',':','\\\\n','\\\\')):\n",
    "\n",
    "                continue\n",
    "\n",
    "            if j=='':\n",
    "\n",
    "                continue\n",
    "\n",
    "            marker=False\n",
    "            word = ''\n",
    "            word_len = len(j)\n",
    "            for k in j:\n",
    "\n",
    "                if j.lower().count(k.lower())>=word_len//2:\n",
    "                    \n",
    "                    continue\n",
    "            \n",
    "                    \n",
    "                if k in punctuation:\n",
    "                    \n",
    "                    continue\n",
    "\n",
    "                if k in nums:\n",
    "                    \n",
    "                    continue\n",
    "                \n",
    "                word+=k\n",
    "\n",
    "                marker=True\n",
    "\n",
    "            if word in all_stops:\n",
    "\n",
    "                continue\n",
    "\n",
    "            if marker==True: \n",
    "\n",
    "                keepers.append(word.lower())\n",
    "    \n",
    "    \n",
    "    return keepers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sol/wordlist.txt','r') as f:\n",
    "    wordlist = [word.rstrip('\\r\\n') for word in  f.readlines()]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = [ word.replace('(p)','').replace('(a)','').replace('(ip)','') for word in wordlist]\n",
    "print len(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print letters,'\\n\\n',punctuation,'\\n\\n',general,'\\n\\n',general_upper,'\\n\\n',general_cap,'\\n\\n',stop,'\\n\\n',stop_upper,'\\n\\n',stop_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for letter in string.ascii_lowercase:\n",
    "    \n",
    "    with open('/home/sol/CLUSTER_RAID/Tweet_Code/Words_By_Alpha/'+letter,'wb') as f:\n",
    "        \n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        for word in wordlist:\n",
    "            \n",
    "            if word.startswith((letter.upper(),letter.lower())):\n",
    "                \n",
    "                writer.writerow([word])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = []\n",
    "for i in range(len(tweets)):\n",
    "    try:\n",
    "        sents.append(json.loads(tweets[i])['text'].encode('ascii', 'ignore'))\n",
    "    except:\n",
    "        continue\n",
    "print len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_set=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sents:\n",
    "    #rint sent\n",
    "    token_set.append(preprocess(sent))\n",
    "\n",
    "print token_set[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned =  Clean_List_of_Sentence(token_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print cleaned[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(cleaned)\n",
    "count_all = Counter(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (word,key),value in count_all.most_common():\n",
    "    \n",
    "    print T[key]+value\n",
    "    \n",
    "    print key,type(value)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['date','possessive pronoun', 'verb present participle or gerund',\n",
    "\n",
    "        'verb past tense', 'verb past participle', 'verb, present tense not 3rd person singular', \n",
    "\n",
    "        'WH-determiner','adjective or numeral, ordinal', 'WH-pronoun', \n",
    "\n",
    "        'verb present tense 3rd person singular','determiner', 'particle', \n",
    "\n",
    "        'noun, common, singular or mass', '\"to\" as preposition or infinitive marker',\n",
    "\n",
    "        'personal pronoun', 'adverb', 'common noun plural', ' proper noun singular',\n",
    "\n",
    "        'base form verb', 'Wh-adverb','coordinating conjunction', 'comparative adverb',\n",
    "\n",
    "        'cardinal numeral', 'No matching tags found', 'existential there there', \n",
    "\n",
    "        'subordinating preposition or conjunction','possessive WH-pronoun', \n",
    "\n",
    "        'modal auxiliary', 'superlative adjective', 'comparative adjective']\n",
    "\n",
    "names = ['PRP$', 'VBG', 'VBD', 'VBN', 'VBP', 'WDT',\n",
    "\n",
    "    'JJ', 'WP', 'VBZ', 'DT', 'RP', 'NN', 'TO',\n",
    "\n",
    "    'PRP', 'RB', 'NNS', 'NNP', 'VB', 'WRB',\n",
    "\n",
    "    'CC', 'RBR', 'CD', '-NONE-', 'EX', 'IN',\n",
    "\n",
    "    'WP$', 'MD', 'JJS', 'JJR' ]\n",
    "\n",
    "T = Table( names=names )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T['NN'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Record_book = {}\n",
    "\n",
    "Record_book.fromkeys(['PRP$', 'VBG', 'VBD', 'VBN', 'VBP', 'WDT',\n",
    "\n",
    "                        'JJ', 'WP', 'VBZ', 'DT', 'RP', 'NN', 'TO',\n",
    "\n",
    "                        'PRP', 'RB', 'NNS', 'NNP', 'VB', 'WRB',\n",
    "\n",
    "                        'CC', 'RBR', 'CD', '-NONE-', 'EX', 'IN',\n",
    "\n",
    "                        'WP$', 'MD', 'JJS', 'JJR' ] , 0 )\n",
    "print Record_book.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print tagged[:50]#tagged[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print count_all.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for adjective,number in count_all.most_common(20):\n",
    "    print word,key,number\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ = ['PRP$', 'VBG', 'VBD', 'VBN', 'VBP', 'WDT', 'JJ', 'WP', 'VBZ', 'DT', 'RP', 'NN', 'TO', \n",
    "\n",
    "'PRP', 'RB', 'NNS', 'NNP', 'VB', 'WRB', 'CC', 'RBR', 'CD', '-NONE-', 'EX', 'IN', 'WP$', 'MD', 'JJS', 'JJR']\n",
    "for symbol in list_:\n",
    "    nltk.help.upenn_tagset(symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word,key,number in count_all.most_common():\n",
    "    \n",
    "    with open(DICTIONARY_PATH+word[0],'r') as word_check:\n",
    "        \n",
    "        if not word in word_check.readlines():\n",
    "            \n",
    "            continue\n",
    "    \n",
    "    d[key] += number\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sol/CLUSTER_RAID/Tweet_Code/dictionary.txt','wb') as f:\n",
    "\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    for key, val in type_dict.items():\n",
    "\n",
    "        writer.writerow([key, val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_log.update(count_all.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_log = {}\n",
    "with open('/home/sol/CLUSTER_RAID/Tweet_Code/dictionary.txt','rb') as f:\n",
    "\n",
    "    for key,val in csv.reader(f):\n",
    "\n",
    "        word_log[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print word_log.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in word_log.keys():\n",
    "    \n",
    "    print key,len(word_log[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammer_book = {}\n",
    "date_book = {}\n",
    "word_book = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammer_book.fromkeys(['DATE_TIME','PRP$', 'VBG', 'VBD', 'VBN', 'VBP', 'WDT', \n",
    "            \n",
    "            'JJ', 'WP', 'VBZ', 'DT', 'RP', 'NN', 'TO', \n",
    "            \n",
    "            'PRP', 'RB', 'NNS', 'NNP', 'VB', 'WRB', 'CC', \n",
    "            \n",
    "            'RBR', 'CD', '-NONE-', 'EX', 'IN', 'WP$', 'MD', \n",
    "            \n",
    "            'JJS', 'JJR'] ,0\n",
    ")\n",
    "word_book = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localtime = time.asctime( time.localtime(time.time()) )\n",
    "type(localtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdtype=(\n",
    "\n",
    "\t\t'f','uint32','uint32','uint32','uint32',\n",
    "\n",
    "\t\t'uint32','uint32','uint32','uint32','uint32',\n",
    "\n",
    "\t\t'uint32','uint32','uint32','uint32','uint32',\n",
    "\n",
    "\t\t'uint32','uint32','uint32','uint32','uint32'\n",
    "\n",
    "\t\t'uint32','uint32','uint32','uint32','uint32',\n",
    "\n",
    "\t\t'uint32','uint32','uint32','uint32','uint32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tnames=(\n",
    "\n",
    "\t\t'date','possessive pronoun', 'verb present participle or gerund',\n",
    "\n",
    "\t\t'verb past tense', 'verb past participle', 'verb, present tense not 3rd person singular', \n",
    "\n",
    "\t\t'WH-determiner','adjective or numeral, ordinal', 'WH-pronoun', \n",
    "\n",
    "\t\t'verb present tense 3rd person singular','determiner', 'particle', \n",
    "\n",
    "\t\t'noun, common, singular or mass', '\"to\" as preposition or infinitive marker',\n",
    "\n",
    "\t\t'personal pronoun', 'adverb', 'common noun plural', ' proper noun singular',\n",
    "\n",
    "\t\t'base form verb', 'Wh-adverb','coordinating conjunction', 'comparative adverb',\n",
    "\n",
    "\t\t'cardinal numeral', 'No matching tags found', 'existential there there', \n",
    "\n",
    "\t\t'subordinating preposition or conjunction','possessive WH-pronoun', \n",
    "\n",
    "\t\t'modal auxiliary', 'superlative adjective', 'comparative adjective')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
